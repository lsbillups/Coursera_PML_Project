---
title: "Coursera_PML Project Report"
author: "lsbillups"
date: "Friday, January 23, 2015"
keep_md: true
output: html_document
---



## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Analysis strategy
The goal of our project is to predict the manner in which they did the exercise. The data available has been already separated into two parts: training data and test data. However, test data is solely for prediction purpose since there's no actually exercise type (i.e. "classe" variable) in the dataset for us to verify the result. Therefore, we have to use cross-validation to generate training and testing set among the original training set.

In this case, I just randomly separate the original training data into two parts (3:1), and use the larger set as the training set (named *training*) and test on the other part of the data (named *testing*). 

The details are as follows.

### 1. Load the data
```{r}
setwd("D:/R/Coursera PML/Project 1")
TotalTraining<-read.csv("pml-training.csv")
TotalTesting<-read.csv("pml-testing.csv")
```

### 2. Separate the training data into two parts
```{r}
library(caret)
set.seed(333)
TraingIndex<-createDataPartition(y=TotalTraining$classe,p=0.75,list=FALSE)
training<-TotalTraining[TraingIndex,]
testing<-TotalTraining[-TraingIndex,]
```

Let's take a look at the training set we generated.

```{r}
dim(training)
```

If we run the *summary* function on the training set, it will show us that there are many missing values in some columns. The result is too long hence omitted.

### 3. Select predictors

First, we exclude the columns with nearly zero variance.
```{r}
novarindex<-nearZeroVar(training)
training<-training[,-novarindex]
```

Next, we get rid of columns with lots of missing values. Here we take the threshold of 50%. That is, if 50% or more entries in this column are missing values, we will exclude this column.

```{r}
lotNA<-function(colvec){
    if (sum(is.na(colvec))/length(colvec)>=0.5){
        return(TRUE)
    }
    else{
        return(FALSE)
    }
}

NAindex<-sapply(training,lotNA)
training<-training[,!NAindex]
```

Finally, we exclude the columns for index,names and time.
```{r}
training<-training[,-seq(from=1,to=6)]
```

And we got the tidy training set we can build our models on.
```{r}
dim(training)
```

### 4.Build the model
In this case, we choose the random forests algorithm since it's one of the commonly used machine learning algorithms and also very accurate. Here, we load the **randomForest** Package and use the function *randomForest* to fit the model.

```{r}
library(randomForest)
modFit<-randomForest(classe ~.,data=training)
print(modFit)
```
### 5. Check error rate

First, we test the in sample error rate. That is, apply the model on the training set and check the error rate.
```{r}
predictontrain<-predict(modFit,training)
print(confusionMatrix(predictontrain,training$classe))
```
As we can see, the model fits the training data extremely well (zero error rate). To make sure the result is not because of overfitting, let's test the out of sample error rate using the testing set.

```{r}
predictontest<-predict(modFit,testing)
print(confusionMatrix(predictontest,testing$classe))
```

The out of sample error rate is 0.4%. Hence we believe in great confidence that our model will make pretty accurate predictions.

### 6. Make prediction

Finally, we make the predictions using the original test dataset and generate text files for testing purposes.

```{r}
finalprediction<-predict(modFit,TotalTesting)
print(finalprediction)


answers<-as.character(finalprediction)
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(answers)
```

